
 @article{Tychola_Kalampokas_Papakostas_2023, title={Quantum Machine Learning—An Overview}, volume={12}, ISSN={2079-9292}, DOI={10.3390/electronics12112379}, abstractNote={Quantum computing has been proven to excel in factorization issues and unordered search problems due to its capability of quantum parallelism. This unique feature allows exponential speedup in solving certain problems. However, this advantage does not apply universally, and challenges arise when combining classical and quantum computing to achieve acceleration in computation speed. This paper aims to address these challenges by exploring the current state of quantum machine learning and benchmarking the performance of quantum and classical algorithms in terms of accuracy. Speciﬁcally, we conducted experiments with three datasets for binary classiﬁcation, implementing Support Vector Machine (SVM) and Quantum SVM (QSVM) algorithms. Our ﬁndings suggest that the QSVM algorithm outperforms classical SVM on complex datasets, and the performance gap between quantum and classical models increases with dataset complexity, as simple models tend to overﬁt with complex datasets. While there is still a long way to go in terms of developing quantum hardware with sufﬁcient resources, quantum machine learning holds great potential in areas such as unsupervised learning and generative models. Moving forward, more efforts are needed to explore new quantum learning models that can leverage the power of quantum mechanics to overcome the limitations of classical machine learning.}, number={11}, journal={Electronics}, author={Tychola, Kyriaki A. and Kalampokas, Theofanis and Papakostas, George A.}, year={2023}, month=may, pages={2379}, language={en} }

@article{webster_analyzing_2002,
	title = {Analyzing the {Past} to {Prepare} for the {Future}: {Writing} a {Literature} {Review}},
	volume = {26},
	url = {http://www.jstor.org/stable/4132319},
	language = {en},
	number = {2},
	journal = {MIS Quarterly},
	author = {Webster, Jane and Watson, Richard T.},
	year = {2002},
	pages = {xiii--xxiii},
	file = {Webster und Watson - 2002 - Analyzing the Past to Prepare for the Future Writ.pdf:C\:\\Users\\rotht\\Zotero\\storage\\BVMK9MHH\\Webster und Watson - 2002 - Analyzing the Past to Prepare for the Future Writ.pdf:application/pdf},
}

@article{fettke_state---art_2006,
	title = {State-of-the-{Art} des {State}-of-the-{Art}: {Eine} {Untersuchung} der {Forschungsmethode} „{Review}” innerhalb der {Wirtschaftsinformatik}},
	volume = {48},
	issn = {0937-6429, 1861-8936},
	shorttitle = {State-of-the-{Art} des {State}-of-the-{Art}},
	url = {http://link.springer.com/10.1007/s11576-006-0057-3},
	doi = {10.1007/s11576-006-0057-3},
	language = {de},
	number = {4},
	urldate = {2022-10-15},
	journal = {WIRTSCHAFTSINFORMATIK},
	author = {Fettke, Peter},
	month = aug,
	year = {2006},
	pages = {257},
	file = {Fettke - 2006 - State-of-the-Art des State-of-the-Art Eine Unters.pdf:C\:\\Users\\rotht\\Zotero\\storage\\KHABRZKB\\Fettke - 2006 - State-of-the-Art des State-of-the-Art Eine Unters.pdf:application/pdf},
}

@article{vom_brocke_reconstructing_nodate,
	title = {{RECONSTRUCTING} {THE} {GIANT}: {ON} {THE} {IMPORTANCE} {OF} {RIGOUR} {IN} {DOCUMENTING} {THE} {LITERATURE} {SEARCH} {PROCESS}},
	abstract = {Science is a cumulative endeavour as new knowledge is often created in the process of interpreting and combining existing knowledge. This is why literature reviews have long played a decisive role in scholarship. The quality of literature reviews is particularly determined by the literature search process. As Sir Isaac Newton eminently put it: “If I can see further, it is because I am standing on the shoulders of giants.” Drawing on this metaphor, the goal of writing a literature review is to reconstruct the giant of accumulated knowledge in a specific domain. And in doing so, a literature search represents the fundamental first step that makes up the giant’s skeleton and largely determines its reconstruction in the subsequent literature analysis. In this paper, we argue that the process of searching the literature must be comprehensibly described. Only then can readers assess the exhaustiveness of the review and other scholars in the field can more confidently (re)use the results in their own research. We set out to explore the methodological rigour of literature review articles published in ten major information systems (IS) journals and show that many of these reviews do not thoroughly document the process of literature search. The results drawn from our analysis lead us to call for more rigour in documenting the literature search process and to present guidelines for crafting a literature review and search in the IS domain.},
	language = {en},
    year = {2009},
	author = {vom Brocke, Jan and Simons, Alexander and Niehaves, Bjoern and Reimer, Kai},
	file = {Simons et al. - RECONSTRUCTING THE GIANT ON THE IMPORTANCE OF RIG.pdf:C\:\\Users\\rotht\\Zotero\\storage\\D8UZIMAP\\Simons et al. - RECONSTRUCTING THE GIANT ON THE IMPORTANCE OF RIG.pdf:application/pdf},
}

@article{amin_hydrogen_2022,
	title = {Hydrogen production through renewable and non-renewable energy processes and their impact on climate change},
	volume = {47},
	issn = {0360-3199},
	url = {https://www.sciencedirect.com/science/article/pii/S0360319922032244},
	doi = {10.1016/j.ijhydene.2022.07.172},
	abstract = {The urbanization and increase in the human population has significantly influenced the global energy demands. The utilization of non-renewable fossil fuel-based energy infrastructure involves air pollution, global warming due to CO2 emissions, greenhouse gas emissions, acid rains, diminishing energy resources, and environmental degradation leading to climate change due to global warming. These factors demand the exploration of alternative energy sources based on renewable sources. Hydrogen, an efficient energy carrier, has emerged as an alternative fuel to meet energy demands and green hydrogen production with zero carbon emission has gained scientific attraction in recent years. This review is focused on the production of hydrogen from renewable sources such as biomass, solar, wind, geothermal, and algae and conventional non-renewable sources including natural gas, coal, nuclear and thermochemical processes. Moreover, the cost analysis for hydrogen production from each source of energy is discussed. Finally, the impact of these hydrogen production processes on the environment and their implications are summarized.},
	number = {77},
	urldate = {2023-09-12},
	journal = {International Journal of Hydrogen Energy},
	author = {Amin, Muhammad and Shah, Hamad Hussain and Fareed, Anaiz Gul and Khan, Wasim Ullah and Chung, Eunhyea and Zia, Adeel and Rahman Farooqi, Zia Ur and Lee, Chaehyeon},
	month = sep,
	year = {2022},
	keywords = {Cost, Environment, Hydrogen, Non-renewable, Renewable},
	pages = {33112--33134},
	file = {ScienceDirect Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\AVAMAR4D\\S0360319922032244.html:text/html},
}

@article{kilkis_research_2019,
	title = {Research frontiers in sustainable development of energy, water and environment systems in a time of climate crisis},
	volume = {199},
	issn = {0196-8904},
	url = {https://www.sciencedirect.com/science/article/pii/S019689041930929X},
	doi = {10.1016/j.enconman.2019.111938},
	abstract = {Sustainable energy conversion and management processes increasingly require an integrated approach, especially in the context of addressing the climate crisis. This editorial puts forth related research frontiers based on 28 research articles of the special issue that is dedicated to the 13th Conference on Sustainable Development of Energy, Water and Environment Systems and regional series based on the 1st Latin American and 3rd South East European Conferences. Seven research frontiers are reviewed, the first three of which are (i) sustainable technologies for local energy systems, (ii) energy storage and advances in flexibility and (iii) solar energy penetration across multiple sectors. These research frontiers contain contributions based on renewable energy for wastewater treatment in islands, energy savings across urban built infrastructure, advanced district heating and cooling networks, power-to-gas and hydrogen production technologies, demand response in industrial systems, hybrid thermal energy storage, hybrid solar energy power plants, novel photovoltaic thermal technologies, and improved solar energy dispatchability. The research frontiers continue with (iv) wind, water based energy and the energy-water nexus, (v) effective valorization and upgrading of resources, (vi) combustion processes and better utilization of heat and (vii) carbon capture, storage and utilization. Significant contributions include innovative wind and hydrokinetic turbines, osmotic power technologies, synergetic solutions for water desalination, efficient catalytic pyrolysis, upgrading to reduce particle pollution, co-processing for alternative fuels, combustion characterization, electricity generation from waste heat sources, advances in heat exchangers and heat transfer, oxy-fuel combustion, post-combustion capture, and fly ash recycling for energy storage material. The research frontiers in this editorial provide ample opportunities to support societal transformations in the next decades to sustain planetary life-support systems.},
	urldate = {2023-09-12},
	journal = {Energy Conversion and Management},
	author = {Kılkış, Şiir and Krajačić, Goran and Duić, Neven and Montorsi, Luca and Wang, Qiuwang and Rosen, Marc A. and Ahmad Al-Nimr, Moh'd},
	month = nov,
	year = {2019},
	keywords = {Environment, Climate mitigation, Energy, Sustainable development, System integration, Water},
	pages = {111938},
	file = {ScienceDirect Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\8QZ7RFT5\\S019689041930929X.html:text/html},
}

@article{yan_innovative_2023,
	title = {Innovative {Electrochemical} {Strategies} for {Hydrogen} {Production}: {From} {Electricity} {Input} to {Electricity} {Output}},
	volume = {62},
	issn = {1433-7851, 1521-3773},
	shorttitle = {Innovative {Electrochemical} {Strategies} for {Hydrogen} {Production}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/anie.202214333},
	doi = {10.1002/anie.202214333},
	abstract = {Renewable H2 production by water electrolysis has attracted much attention due to its numerous advantages. However, the energy consumption of conventional water electrolysis is high and mainly driven by the kinetically inert anodic oxygen evolution reaction. An alternative approach is the coupling of different half-cell reactions and the use of redox mediators. In this review, we, therefore, summarize the latest findings on innovative electrochemical strategies for H2 production. First, we address redox mediators utilized in water splitting, including soluble and insoluble species, and the corresponding cell concepts. Second, we discuss alternative anodic reactions involving organic and inorganic chemical transformations. Then, electrochemical H2 production at both the cathode and anode, or even H2 production together with electricity generation, is presented. Finally, the remaining challenges and prospects for the future development of this research field are highlighted.},
	language = {en},
	number = {16},
	urldate = {2023-09-12},
	journal = {Angewandte Chemie International Edition},
	author = {Yan, Dafeng and Mebrahtu, Chalachew and Wang, Shuangyin and Palkovits, Regina},
	month = apr,
	year = {2023},
	pages = {e202214333},
	file = {Yan et al. - 2023 - Innovative Electrochemical Strategies for Hydrogen.pdf:C\:\\Users\\rotht\\Zotero\\storage\\3Q4286BJ\\Yan et al. - 2023 - Innovative Electrochemical Strategies for Hydrogen.pdf:application/pdf},
}

@article{chen_waste-derived_2023,
	title = {Waste-{Derived} {Catalysts} for {Water} {Electrolysis}: {Circular} {Economy}-{Driven} {Sustainable} {Green} {Hydrogen} {Energy}},
	volume = {15},
	issn = {2311-6706, 2150-5551},
	shorttitle = {Waste-{Derived} {Catalysts} for {Water} {Electrolysis}},
	url = {https://link.springer.com/10.1007/s40820-022-00974-7},
	doi = {10.1007/s40820-022-00974-7},
	abstract = {The sustainable production of green hydrogen via water electrolysis necessitates cost-effective electrocatalysts. By following the circular economy principle, the utilization of waste-derived catalysts sig‑nificantly promotes the sustainable development of green hydrogen energy. Currently, diverse waste-derived catalysts have exhibited excellent catalytic performance toward hydrogen evolution reaction (HER), oxygen evolution reaction (OER), and overall water electrolysis (OWE). Herein, we system‑atically examine recent achievements in waste-derived electrocatalysts for water electrolysis. The general principles of water electrolysis and design principles of efficient electrocatalysts are discussed, followed by the illustra‑tion of current strategies for transforming wastes into electrocatalysts. Then, applications of waste-derived catalysts (i.e., carbon-based catalysts, transi‑tional metal-based catalysts, and carbon-based heterostructure catalysts) in HER, OER, and OWE are reviewed successively. An emphasis is put on cor‑relating the catalysts’ structure–performance relationship. Also, challenges and research directions in this booming field are finally highlighted. This review would provide useful insights into the design, synthesis, and applications of waste-derived electrocatalysts, and thus accelerate the development of the circular economy-driven green hydrogen energy scheme.},
	language = {en},
	number = {1},
	urldate = {2023-09-12},
	journal = {Nano-Micro Letters},
	author = {Chen, Zhijie and Yun, Sining and Wu, Lan and Zhang, Jiaqi and Shi, Xingdong and Wei, Wei and Liu, Yiwen and Zheng, Renji and Han, Ning and Ni, Bing-Jie},
	month = dec,
	year = {2023},
	pages = {4},
	file = {Chen et al. - 2023 - Waste-Derived Catalysts for Water Electrolysis Ci.pdf:C\:\\Users\\rotht\\Zotero\\storage\\HUUFM7TD\\Chen et al. - 2023 - Waste-Derived Catalysts for Water Electrolysis Ci.pdf:application/pdf},
}

@article{tran_open_2023,
	title = {The {Open} {Catalyst} 2022 ({OC22}) {Dataset} and {Challenges} for {Oxide} {Electrocatalysts}},
	volume = {13},
	issn = {2155-5435, 2155-5435},
	url = {http://arxiv.org/abs/2206.08917},
	doi = {10.1021/acscatal.2c05426},
	abstract = {The development of machine learning models for electrocatalysts requires a broad set of training data to enable their use across a wide variety of materials. One class of materials that currently lacks sufficient training data is oxides, which are critical for the development of OER catalysts. To address this, we developed the OC22 dataset, consisting of 62,331 DFT relaxations ({\textasciitilde}9,854,504 single point calculations) across a range of oxide materials, coverages, and adsorbates. We define generalized total energy tasks that enable property prediction beyond adsorption energies; we test baseline performance of several graph neural networks; and we provide pre-defined dataset splits to establish clear benchmarks for future efforts. In the most general task, GemNet-OC sees a {\textasciitilde}36\% improvement in energy predictions when combining the chemically dissimilar OC20 and OC22 datasets via fine-tuning. Similarly, we achieved a {\textasciitilde}19\% improvement in total energy predictions on OC20 and a {\textasciitilde}9\% improvement in force predictions in OC22 when using joint training. We demonstrate the practical utility of a top performing model by capturing literature adsorption energies and important OER scaling relationships. We expect OC22 to provide an important benchmark for models seeking to incorporate intricate long-range electrostatic and magnetic interactions in oxide surfaces. Dataset and baseline models are open sourced, and a public leaderboard is available to encourage continued community developments on the total energy tasks and data.},
	number = {5},
	urldate = {2023-09-12},
	journal = {ACS Catalysis},
	author = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Felix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
	month = mar,
	year = {2023},
	note = {arXiv:2206.08917 [cond-mat, physics:physics]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science, Physics - Computational Physics},
	pages = {3066--3084},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\XQX97FLC\\2206.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\LWZUN8UI\\Tran et al. - 2023 - The Open Catalyst 2022 (OC22) Dataset and Challeng.pdf:application/pdf},
}

@misc{verdon_quantum_2019,
	title = {Quantum {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1909.12264},
	abstract = {We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Verdon, Guillaume and McCourt, Trevor and Luzhnica, Enxhell and Singh, Vikash and Leichenauer, Stefan and Hidary, Jack},
	month = sep,
	year = {2019},
	note = {arXiv:1909.12264 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\6EKDMLQ2\\1909.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\4N6XH2KC\\Verdon et al. - 2019 - Quantum Graph Neural Networks.pdf:application/pdf},
}

@misc{beer_quantum_2021,
	title = {Quantum machine learning of graph-structured data},
	url = {http://arxiv.org/abs/2103.10837},
	abstract = {Graph structures are ubiquitous throughout the natural sciences. Here we consider graph-structured quantum data and describe how to carry out its quantum machine learning via quantum neural networks. In particular, we consider training data in the form of pairs of input and output quantum states associated with the vertices of a graph, together with edges encoding correlations between the vertices. We explain how to systematically exploit this additional graph structure to improve quantum learning algorithms. These algorithms are numerically simulated and exhibit excellent learning behavior. Scalable quantum implementations of the learning procedures are likely feasible on the next generation of quantum computing devices.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Beer, Kerstin and Khosla, Megha and Köhler, Julius and Osborne, Tobias J.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.10837 [quant-ph]},
	keywords = {Quantum Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\TAFKFSGX\\2103.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\ASU8LYV5\\Beer et al. - 2021 - Quantum machine learning of graph-structured data.pdf:application/pdf},
}

@misc{ai_decompositional_2023,
	title = {Decompositional {Quantum} {Graph} {Neural} {Network}},
	url = {http://arxiv.org/abs/2201.05158},
	abstract = {Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This mapping maintains the distance relations present in the data and reduces information loss. Experimental results show that the proposed method outperforms competitive state-of-the-art models with only 1.68{\textbackslash}\% parameters compared to those models.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Ai, Xing and Zhang, Zhihong and Sun, Luzhe and Yan, Junchi and Hancock, Edwin},
	month = mar,
	year = {2023},
	note = {arXiv:2201.05158 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\V5HX87UG\\2201.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\DGA994XR\\Ai et al. - 2023 - Decompositional Quantum Graph Neural Network.pdf:application/pdf},
}

@article{peffers_design_2007,
	title = {A {Design} {Science} {Research} {Methodology} for {Information} {Systems} {Research}},
	volume = {24},
	issn = {0742-1222},
	url = {https://doi.org/10.2753/MIS0742-1222240302},
	doi = {10.2753/MIS0742-1222240302},
	abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
	number = {3},
	urldate = {2023-09-12},
	journal = {Journal of Management Information Systems},
	author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
	month = dec,
	year = {2007},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.2753/MIS0742-1222240302},
	keywords = {case study, design science, design science research, design theory, mental model, methodology, process model},
	pages = {45--77},
}

@inproceedings{wirth2000crisp,
  title={CRISP-DM: Towards a standard process model for data mining},
  author={Wirth, R{\"u}diger and Hipp, Jochen},
  booktitle={Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining},
  volume={1},
  pages={29--39},
  year={2000},
  organization={Manchester}
}

@misc{noauthor_torch_geometricdatasetsqm9_nodate,
	author = {{PyTorch Geometric Documentation - Torch geometric datasets}},
	title = {QM9 Dataset Description},
	url = {https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.QM9.html},
	urldate = {2023-11-22},
	year = {2023}
}

@misc{gasteiger_directional_2022,
	title = {Directional {Message} {Passing} for {Molecular} {Graphs}},
	url = {http://arxiv.org/abs/2003.03123},
	abstract = {Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76\% on MD17 and by 31\% on QM9. Our implementation is available online.},
	urldate = {2023-11-21},
	publisher = {arXiv},
	author = {Gasteiger, Johannes and Groß, Janek and Günnemann, Stephan},
	month = apr,
	year = {2022},
	note = {arXiv:2003.03123 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2020. Author name changed from Johannes Klicpera to Johannes Gasteiger},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\YFIDPIPH\\2003.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\2YARLY4S\\Gasteiger et al. - 2022 - Directional Message Passing for Molecular Graphs.pdf:application/pdf},
}

@misc{noauthor_deepchem_nodate,
	title = {{DeepChem Documentation}},
	url = {https://deepchem.io/about/},
	urldate = {2023-11-27},
	year = {2023},
	file = {DeepChem:C\:\\Users\\rotht\\Zotero\\storage\\JEUN2JEU\\about.html:text/html},
}

@article{altae-tran_low_2017,
	title = {Low {Data} {Drug} {Discovery} with {One}-{Shot} {Learning}},
	volume = {3},
	issn = {2374-7943, 2374-7951},
	url = {https://pubs.acs.org/doi/10.1021/acscentsci.6b00367},
	doi = {10.1021/acscentsci.6b00367},
	language = {en},
	number = {4},
	urldate = {2023-11-27},
	journal = {ACS Central Science},
	author = {Altae-Tran, Han and Ramsundar, Bharath and Pappu, Aneesh S. and Pande, Vijay},
	month = apr,
	year = {2017},
	pages = {283--293},
	file = {Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\H6CZR37B\\Altae-Tran et al. - 2017 - Low Data Drug Discovery with One-Shot Learning.pdf:application/pdf},
}

@article{xiong_pushing_2019,
	title = {Pushing the boundaries of molecular representation for drug discovery with graph attention mechanism},
	volume = {63},
	doi = {10.1021/acs.jmedchem.9b00959},
	abstract = {Hunting for chemicals with favourable pharmacological, toxicological and pharmacokinetic properties remains a formidable challenge for drug discovery. Deep learning provides us with powerful tools to build predictive models that are appropriate for the rising amounts of data, but the gap between what these neural networks learn and what human beings can comprehend is growing. Moreover, this gap may induce distrust and restrict deep learning applications in practice. Here, we introduce a new graph neural network architecture called Attentive FP for molecular representation that uses a graph attention mechanism to learn from relevant drug discovery datasets. We demonstrate that Attentive FP achieves state-of-the-art predictive performances on a variety of datasets and that what it learns is interpretable. The feature visualization for Attentive FP suggests that it automatically learns non-local intramolecular interactions from specified tasks, which can help us gain chemical insights directly from data beyond human perception.},
	journal = {Journal of Medicinal Chemistry},
	author = {Xiong, Zhaoping and Wang, Dingyan and Liu, Xiaohong and Feisheng, Zhong and Wan, Xiaozhe and Li, Xutong and Li, Zhaojun and Luo, Xiaomin and Chen, Kaixian and Jiang, H. and Zheng, Mingyue},
	month = aug,
	year = {2019},
}

@article{jiang_could_2021,
	title = {Could graph neural networks learn better molecular representation for drug discovery? {A} comparison study of descriptor-based and graph-based models},
	volume = {13},
	issn = {1758-2946},
	shorttitle = {Could graph neural networks learn better molecular representation for drug discovery?},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00479-8},
	doi = {10.1186/s13321-020-00479-8},
	abstract = {Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property predic‑tion, and numerous studies have shown that GNN could yield more promising results than traditional descriptorbased methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algo‑rithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reli‑able predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and dem‑onstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptorbased models still can be directly employed to accurately predict various chemical endpoints with excellent comput‑ability and interpretability.},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {Journal of Cheminformatics},
	author = {Jiang, Dejun and Wu, Zhenxing and Hsieh, Chang-Yu and Chen, Guangyong and Liao, Ben and Wang, Zhe and Shen, Chao and Cao, Dongsheng and Wu, Jian and Hou, Tingjun},
	month = feb,
	year = {2021},
	pages = {12},
	file = {Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf:C\:\\Users\\rotht\\Zotero\\storage\\Q7WD35CS\\Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf:application/pdf},
}


@inproceedings{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {https://proceedings.mlr.press/v70/gilmer17a.html},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	language = {en},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1263--1272},
	file = {Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\IYWIUZ3G\\Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2023-11-21},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\S72IKL8Y\\1810.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\I7REEX99\\Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf:application/pdf},
}


@article{velickovic_everything_2023,
	title = {Everything is {Connected}: {Graph} {Neural} {Networks}},
	volume = {79},
	issn = {0959440X},
	shorttitle = {Everything is {Connected}},
	url = {http://arxiv.org/abs/2301.08210},
	doi = {10.1016/j.sbi.2023.102538},
	abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
	urldate = {2023-11-21},
	journal = {Current Opinion in Structural Biology},
	author = {Veličković, Petar},
	month = apr,
	year = {2023},
	note = {arXiv:2301.08210 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	pages = {102538},
	annote = {Comment: To appear in Current Opinion in Structural Biology. 14 pages, 1 figure},
	file = {arXiv.org Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\2VZUM672\\2301.html:text/html;Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\P54AM6ZA\\Veličković - 2023 - Everything is Connected Graph Neural Networks.pdf:application/pdf},
}

@article{liu_computational_2023,
	title = {Computational {Prediction} of {Graphdiyne}-{Supported} {Three}-{Atom} {Single}-{Cluster} {Catalysts}},
	volume = {5},
	issn = {2096-5745},
	url = {http://www.chinesechemsoc.org/doi/10.31635/ccschem.022.202201796},
	doi = {10.31635/ccschem.022.202201796},
	language = {en},
	number = {1},
	urldate = {2023-11-21},
	journal = {CCS Chemistry},
	author = {Liu, Jin-Cheng and Xiao, Hai and Zhao, Xiao-Kun and Zhang, Nan-Nan and Liu, Yuan and Xing, Deng-Hui and Yu, Xiaohu and Hu, Han-Shi and Li, Jun},
	month = jan,
	year = {2023},
	pages = {152--163},
	file = {Full Text PDF:C\:\\Users\\rotht\\Zotero\\storage\\22UJKYHA\\Liu et al. - 2023 - Computational Prediction of Graphdiyne-Supported T.pdf:application/pdf},
}

@article{wu_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9046288/},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classiﬁcation and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed signiﬁcant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning ﬁelds. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2023-11-21},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	month = jan,
	year = {2021},
	pages = {4--24},
	file = {Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf:C\:\\Users\\rotht\\Zotero\\storage\\XBI7V6VW\\Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf:application/pdf},
}
@article{zhang2019graph,
  title={Graph convolutional networks: a comprehensive review},
  author={Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
  journal={Computational Social Networks},
  volume={6},
  number={1},
  pages={1--23},
  year={2019},
  publisher={SpringerOpen}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {26666510},
	shorttitle = {Graph neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	language = {en},
	urldate = {2023-11-21},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2020},
	pages = {57--81},
	file = {Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf:C\:\\Users\\rotht\\Zotero\\storage\\NBTB4JSU\\Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf:application/pdf},
}

@article{li2017diffusion,
  title={Diffusion convolutional recurrent neural network: Data-driven traffic forecasting},
  author={Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
  journal={arXiv preprint arXiv:1707.01926},
  year={2017}
}

@article{hajiramezanali2019variational,
  title={Variational graph recurrent neural networks},
  author={Hajiramezanali, Ehsan and Hasanzadeh, Arman and Narayanan, Krishna and Duffield, Nick and Zhou, Mingyuan and Qian, Xiaoning},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2021adaptive,
  title={Adaptive graph auto-encoder for general data clustering},
  author={Li, Xuelong and Zhang, Hongyuan and Zhang, Rui},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={12},
  pages={9725--9732},
  year={2021},
  publisher={IEEE}
}

@article{zhang2021graph,
  title={Graph neural networks and their current applications in bioinformatics},
  author={Zhang, Xiao-Meng and Liang, Li and Liu, Lin and Tang, Ming-Jing},
  journal={Frontiers in genetics},
  volume={12},
  pages={690049},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{menonunderstanding,
  title={Understanding Molecular Data using Graph Convolutional Networks},
  author={Menon, Suraj}
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://pytorch.org/},
	language = {en},
	urldate = {2023-11-28},
	journal = {PyTorch},
	file = {Snapshot:C\:\\Users\\rotht\\Zotero\\storage\\B9LMX2UR\\pytorch.org.html:text/html},
}

@inproceedings{zheng2021quantum,
  title={Quantum graph convolutional neural networks},
  author={Zheng, Jin and Gao, Qing and L{\"u}, Yanxuan},
  booktitle={2021 40th Chinese Control Conference (CCC)},
  pages={6335--6340},
  year={2021},
  organization={IEEE}
}

@article{ryu2023quantum,
  title={Quantum Graph Neural Network Models for Materials Search},
  author={Ryu, Ju-Young and Elala, Eyuel and Rhee, June-Koo Kevin},
  journal={Materials},
  volume={16},
  number={12},
  pages={4300},
  year={2023},
  publisher={MDPI}
}

@article{claudino2022basics,
  title={The basics of quantum computing for chemists},
  author={Claudino, Daniel},
  journal={International Journal of Quantum Chemistry},
  volume={122},
  number={23},
  pages={e26990},
  year={2022},
  publisher={Wiley Online Library}
}