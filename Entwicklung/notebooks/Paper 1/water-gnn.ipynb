{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import QM9\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "# import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define a simple function to convert atom types to atomic numbers\n",
    "def atom_to_atomic_number(atom):\n",
    "    mapping = {'H': 1, 'O': 0}\n",
    "    return mapping.get(atom, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    dataset = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if lines[i].startswith('Energy'):\n",
    "            # Extract energy\n",
    "            energy = float(lines[i].split()[-1])\n",
    "            i += 1  # Move to the next line\n",
    "\n",
    "            atom_types = []\n",
    "            positions = []\n",
    "            while i < len(lines) and not lines[i].startswith('Energy'):\n",
    "                parts = lines[i].split()\n",
    "                if len(parts) == 4:  # Ensure the line is for an atom\n",
    "                    atom_types.append(atom_to_atomic_number(parts[0]))\n",
    "                    positions.append([float(part) for part in parts[1:]])\n",
    "                i += 1\n",
    "\n",
    "            # Convert to PyTorch tensors\n",
    "            atom_types_tensor = torch.tensor(atom_types, dtype=torch.long)\n",
    "            positions_tensor = torch.tensor(positions, dtype=torch.float)\n",
    "            energy_tensor = torch.tensor([energy], dtype=torch.float)\n",
    "\n",
    "            # Create a Data object\n",
    "            data = Data(x=atom_types_tensor, pos=positions_tensor, y=energy_tensor)\n",
    "            dataset.append(data)\n",
    "        else:\n",
    "            i += 1  # Skip lines that don't start with 'Energy'\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = parse_dataset('water-DZ-F12-STATIC-g32n10-3M_PES-DZERO.xyz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3], y=[1], pos=[3, 3])\n",
      "tensor([0, 1, 1])\n",
      "tensor([[ 0.0089,  0.0000,  0.4355],\n",
      "        [-0.7973,  0.0000, -0.5977],\n",
      "        [ 0.6555,  0.0000, -0.4876]])\n"
     ]
    }
   ],
   "source": [
    "for d in dataset:\n",
    "    print(d)\n",
    "    print(d.x)  # Here, d.x is used instead of d.z; you might need to adjust based on your exact needs\n",
    "    print(d.pos)\n",
    "    break  # Remove this to loop through the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element one hots: \n",
      " tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], dtype=torch.float64)\n",
      "tensor size of elements:  3\n",
      "Coordinates: \n",
      " [[ 0.0089378   0.          0.43550962]\n",
      " [-0.79730093  0.         -0.5976605 ]\n",
      " [ 0.6554516   0.         -0.48758563]]\n",
      "Label: tensor([-76.2732])\n"
     ]
    }
   ],
   "source": [
    "# Now we will do some processing of the data to get into a more usable format. \n",
    "# Let’s convert to numpy arrays, remove the partial charges, and convert the elements into one-hot vectors.\n",
    "def convert_record(d):\n",
    "    # elements\n",
    "    e = d.x #.numpy()\n",
    "    # xyz position\n",
    "    x = d.pos.numpy() # das hier ist Numpy aber wird später zu tensor (bei x2e)\n",
    "    # target \n",
    "    y = d.y\n",
    "\n",
    "    # make ohc size larger\n",
    "    # so use same node feature\n",
    "    # shape later\n",
    "    ohc = np.zeros((len(e), 2))\n",
    "    ohc[np.arange(len(e)), e - 1] = 1\n",
    "    ohc_tensor = torch.from_numpy(ohc)\n",
    "    return (ohc_tensor, x), y\n",
    "\n",
    "for d in dataset:\n",
    "    (e, x), y = convert_record(d)\n",
    "    print(\"Element one hots: \\n\", e)\n",
    "    print(\"tensor size of elements: \", e.size()[0])\n",
    "    print(\"Coordinates: \\n\", x)\n",
    "    print(\"Label:\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.random_split(dataset, [0.05,0.01,0.01,0.93])\n",
    "training_set = dataset[0]\n",
    "test_set = dataset[1]\n",
    "validation_set = dataset[2]\n",
    "# dataset[3] = all remaining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean =  -76.16135 Std = 0.2403025\n"
     ]
    }
   ],
   "source": [
    "ys = [convert_record(d)[1].numpy() for d in training_set]\n",
    "train_ym = np.mean(ys)\n",
    "train_ys = np.std(ys)\n",
    "print(\"Mean = \", train_ym, \"Std =\", train_ys)\n",
    "\n",
    "\n",
    "def transform_label(y):\n",
    "    return (y - train_ym) / train_ys\n",
    "\n",
    "\n",
    "def transform_prediction(y):\n",
    "    return y * train_ys + train_ym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5823, 0.7873],\n",
      "        [0.5823, 0.0000, 0.4711],\n",
      "        [0.7873, 0.4711, 0.0000]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotht\\AppData\\Local\\Temp/ipykernel_15900/3558362042.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  e = np.where(r2 != 0, 1 / r2, 0.0)\n"
     ]
    }
   ],
   "source": [
    "def x2e(x):\n",
    "    \"\"\"convert xyz coordinates to inverse pairwise distance\"\"\"\n",
    "    r2 = np.sum((x - x[:, np.newaxis, :]) ** 2, axis=-1)\n",
    "    e = np.where(r2 != 0, 1 / r2, 0.0)\n",
    "    e = torch.from_numpy(e).double()\n",
    "    return e\n",
    "\n",
    "print(x2e(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_layer(nodes, edges, features, we, wb, wv, wu):\n",
    "\n",
    "    leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "    new_nodes_tensor = torch.repeat_interleave(nodes[None, :], nodes.size()[0], dim=0)\n",
    "    input_tensor = wb + torch.matmul(new_nodes_tensor ,we) * torch.transpose(edges[:,None],1,2)\n",
    "    ek = leaky_relu(input_tensor)\n",
    "\n",
    "    # sum over neighbors to get N x features\n",
    "    ebar = torch.sum(ek, 1)\n",
    "\n",
    "    # dense layer for new nodes to get N x features\n",
    "    new_nodes = leaky_relu(torch.matmul(ebar, wv) + nodes)\n",
    "\n",
    "    # sum over nodes to get shape features\n",
    "    global_node_features = torch.sum(new_nodes,0)    \n",
    "\n",
    "    # dense layer for new features\n",
    "    new_features = leaky_relu(torch.matmul(global_node_features, wu)) + features\n",
    "\n",
    "    # just return features for ease of use\n",
    "    return new_nodes, edges, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: \n",
      " tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], dtype=torch.float64)\n",
      "edges: \n",
      " tensor([[0.0000, 0.5823, 0.7873],\n",
      "        [0.5823, 0.0000, 0.4711],\n",
      "        [0.7873, 0.4711, 0.0000]], dtype=torch.float64)\n",
      "features: \n",
      " tensor([0., 0.], dtype=torch.float64)\n",
      "input feautres: \n",
      " tensor([0., 0.], dtype=torch.float64)\n",
      "output features: \n",
      " tensor([-0.0005,  0.1555], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotht\\AppData\\Local\\Temp/ipykernel_15900/3558362042.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  e = np.where(r2 != 0, 1 / r2, 0.0)\n"
     ]
    }
   ],
   "source": [
    "graph_feature_len = 2\n",
    "node_feature_len = 2\n",
    "msg_feature_len = 9\n",
    "\n",
    "# make our weights as tensors\n",
    "def init_weights(g, n, m):\n",
    "\n",
    "    we = torch.nn.Parameter(data=torch.Tensor(n,m).double(), requires_grad=True) \n",
    "    we.data.normal_(0,1e-1)\n",
    "   \n",
    "    wb = torch.nn.Parameter(data=torch.Tensor(m).double(), requires_grad=True) \n",
    "    wb.data.normal_(0,1e-1)\n",
    "\n",
    "    wv = torch.nn.Parameter(data=torch.Tensor(m,n).double(), requires_grad=True) \n",
    "    wv.data.normal_(0,1e-1)\n",
    "\n",
    "    wu = torch.nn.Parameter(data=torch.Tensor(n,g).double(), requires_grad=True) \n",
    "    wu.data.normal_(0,1e-1)\n",
    "    \n",
    "    return [we, wb, wv, wu]\n",
    "\n",
    "# make a graph\n",
    "nodes = e\n",
    "edges = x2e(x)\n",
    "features = np.zeros(graph_feature_len)\n",
    "features = torch.from_numpy(features).double()\n",
    "\n",
    "print(\"nodes: \\n\", nodes)\n",
    "print(\"edges: \\n\", edges)\n",
    "print(\"features: \\n\", features)\n",
    "\n",
    "# eval\n",
    "weights = init_weights(graph_feature_len, node_feature_len, msg_feature_len)\n",
    "out = gnn_layer(nodes, edges, features, *weights)\n",
    "\n",
    "# print(\"weights: \\n\", weights)\n",
    "print(\"input feautres: \\n\", features)\n",
    "print(\"output features: \\n\", out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1098], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotht\\AppData\\Local\\Temp/ipykernel_15900/3558362042.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  e = np.where(r2 != 0, 1 / r2, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# get weights for both layers\n",
    "w1 = init_weights(graph_feature_len, node_feature_len, msg_feature_len)\n",
    "w2 = init_weights(graph_feature_len, node_feature_len, msg_feature_len) #! ausprobieren, kann sein, dass Variablen überschrieben werden!!!\n",
    "[we1, wb1, wv1, wu1] = w1\n",
    "[we2, wb2, wv2, wu2] = w2\n",
    "\n",
    "w3 = torch.nn.Parameter(data=torch.Tensor(graph_feature_len).double(), requires_grad=True)\n",
    "w3.data.normal_(0,1)\n",
    "\n",
    "b = torch.nn.Parameter(data=torch.zeros(1).double(), requires_grad=True) # b wird mit einer Null initialisiert\n",
    "\n",
    "class Model(nn.Module):\n",
    "    # funktion für initialisierung hier einbauen\n",
    "\n",
    "    def __init__(self, w1, w2, w3, b, graph_feature_len): # hier argumente für die layer\n",
    "        # Gewichte werden hier initialisiert !!! Nicht übergeben\n",
    "        # hier kann ich gnn_layer klasse aufrufen!\n",
    "        # self.gnn_layer = # instanz initialisieren\n",
    "        super(Model, self).__init__()\n",
    "        self.w1= w1\n",
    "        self.w2 = w2\n",
    "        self.w3 = w3\n",
    "        self.b = b\n",
    "        self.graph_feature_len = graph_feature_len\n",
    "\n",
    "    def gnn_layer(self, nodes, edges, features, we, wb, wv, wu): # separate Klasse die auch von nn.module erbt!\n",
    "        \"\"\" Implementation of the GNN\n",
    "        make nodes be N x N so we can just multiply directly\n",
    "        ek is now shaped N x N x features \"\"\"\n",
    "        # ek = jax.nn.leaky_relu(web + jnp.repeat(nodes[jnp.newaxis, ...], nodes.shape[0], axis=0) @ we * edges[..., jnp.newaxis]\n",
    "        leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "        new_nodes_tensor = torch.repeat_interleave(nodes[None, :], nodes.size()[0], dim=0)\n",
    "        input_tensor = wb + torch.matmul(new_nodes_tensor ,we) * torch.transpose(edges[:,None],1,2)\n",
    "        ek = leaky_relu(input_tensor)\n",
    "        # sum over neighbors to get N x features\n",
    "        # ebar = jnp.sum(ek, axis=1)\n",
    "        ebar = torch.sum(ek, 1)\n",
    "        # dense layer for new nodes to get N x features\n",
    "        # new_nodes = jax.nn.leaky_relu(ebar @ wv) + nodes\n",
    "        new_nodes = leaky_relu(torch.matmul(ebar, wv) + nodes)\n",
    "        # sum over nodes to get shape features\n",
    "        # global_node_features = jnp.sum(new_nodes, axis=0)   \n",
    "        global_node_features = torch.sum(new_nodes,0)    \n",
    "        # dense layer for new features\n",
    "        # new_features = jax.nn.leaky_relu(global_node_features @ wu) + features\n",
    "        new_features = leaky_relu(torch.matmul(global_node_features, wu)) + features\n",
    "        # just return features for ease of use\n",
    "        return new_nodes, edges, new_features\n",
    "\n",
    "    def forward(self, nodes, coords):\n",
    "        f0 = torch.from_numpy(np.zeros(self.graph_feature_len)).double()\n",
    "        e0 = x2e(coords)\n",
    "        n0 = nodes\n",
    "        n1, e1, f1 = gnn_layer(n0, e0, f0, *self.w1)\n",
    "        n2, e2, f2 = gnn_layer(n1, e1, f1, *self.w2)\n",
    "        yhat = torch.matmul(f2, self.w3) + self.b\n",
    "        return yhat\n",
    "\n",
    "def loss(nodes, coords, y, w1, w2, w3, b):\n",
    "    return (Model(nodes, coords, w1, w2, w3, b) - y) ** 2\n",
    "\n",
    "def loss1(nodes, coords, y, w1, w2, w3, b, graph_feature_len):\n",
    "    model = Model(w1, w2, w3, b, graph_feature_len)\n",
    "    model.eval()\n",
    "    prediction = model(nodes, coords)\n",
    "    return (model(nodes, coords) - y) ** 2\n",
    "\n",
    "model = Model(w1, w2, w3, b, graph_feature_len)\n",
    "model.eval()\n",
    "print(model(nodes, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(os.getcwd(),'water_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotht\\AppData\\Local\\Temp/ipykernel_15900/3558362042.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  e = np.where(r2 != 0, 1 / r2, 0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optim = torch.optim.Adam([we1, wb1, wv1, wu1, we2, wb2, wv2, wu2, w3, b], lr=1e-3)\n",
    "# arg1: über welche Parameter optimiert werden soll (Liste mit allen Parametern: *w1,*w2,w3, lr = eta (1e-3))\n",
    "\n",
    "epochs = 16\n",
    "eta = 1e-3\n",
    "val_loss = [0.0 for _ in range(epochs)]\n",
    "for epoch in range(epochs):\n",
    "    for d in training_set:\n",
    "        (e, x), y_raw = convert_record(d)\n",
    "        y = transform_label(y_raw)\n",
    "        # originally in jax:\n",
    "        # grad = loss_grad(e, x, y, w1, w2, w3, b) # defined above in jax\n",
    "        compute_loss = loss1(e, x, y, w1, w2, w3, b, graph_feature_len) # ursprünglich loss statt loss1\n",
    "        \n",
    "        optim.zero_grad() # vor backward gradienten null gesetzt, sonst Akkumulation vom vorherigen Schritt! (exploding/vanishing gradients)\n",
    "        compute_loss.backward()\n",
    "        optim.step()\n",
    "        # * * *\n",
    "\n",
    "    # compute validation loss\n",
    "    for v in validation_set:\n",
    "        (e, x), y_raw = convert_record(v)\n",
    "        y = transform_label(y_raw)\n",
    "        # convert SE to RMSE\n",
    "    val_loss[epoch] += loss1(e, x, y, w1, w2, w3, b, graph_feature_len) # ursprünglich loss statt loss1 !\n",
    "    val_loss[epoch] = torch.sqrt(val_loss[epoch] / 1000)\n",
    "    eta *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'water_model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotht\\AppData\\Local\\Temp/ipykernel_15900/3558362042.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  e = np.where(r2 != 0, 1 / r2, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ys:  [tensor([-76.1391]), tensor([-76.2413]), tensor([-75.8920]), tensor([-76.2737]), tensor([-76.2905]), tensor([-76.3117]), tensor([-76.3470]), tensor([-76.2195]), tensor([-76.3028]), tensor([-76.2891]), tensor([-76.2059]), tensor([-76.2785]), tensor([-76.3269]), tensor([-76.3218]), tensor([-76.3249]), tensor([-76.3312]), tensor([-76.2373]), tensor([-76.2835]), tensor([-76.2622]), tensor([-76.2593]), tensor([-76.2471]), tensor([-76.2490]), tensor([-76.1140]), tensor([-75.5222]), tensor([-75.6879]), tensor([-76.2754]), tensor([-75.7625]), tensor([-76.0040]), tensor([-76.2856]), tensor([-76.1749]), tensor([-76.2125]), tensor([-76.2463]), tensor([-76.1570]), tensor([-76.1068]), tensor([-76.1761]), tensor([-76.3001]), tensor([-76.2875]), tensor([-76.2813]), tensor([-76.3200]), tensor([-76.3072]), tensor([-76.2806]), tensor([-76.2430]), tensor([-76.2513]), tensor([-76.2719]), tensor([-76.2362]), tensor([-76.2100]), tensor([-76.2399]), tensor([-76.2407]), tensor([-76.0136]), tensor([-76.2706]), tensor([-76.3425]), tensor([-76.1487]), tensor([-76.3257]), tensor([-76.2154]), tensor([-76.3340]), tensor([-76.2069]), tensor([-76.3093]), tensor([-76.2524]), tensor([-76.3153]), tensor([-76.2499]), tensor([-76.2173]), tensor([-76.2353]), tensor([-76.3248]), tensor([-76.0378]), tensor([-76.2874]), tensor([-76.2160]), tensor([-76.2172]), tensor([-76.2104]), tensor([-76.2839]), tensor([-76.2374]), tensor([-76.2469]), tensor([-76.3094]), tensor([-76.2012]), tensor([-76.2500]), tensor([-76.1616]), tensor([-76.2275]), tensor([-75.9876]), tensor([-76.2323]), tensor([-76.3005]), tensor([-76.1791]), tensor([-76.2557]), tensor([-76.2862]), tensor([-76.3096]), tensor([-75.3936]), tensor([-76.3496]), tensor([-76.1906]), tensor([-76.2704]), tensor([-76.2464]), tensor([-76.3046]), tensor([-76.2766]), tensor([-76.2730]), tensor([-75.9150]), tensor([-76.3101]), tensor([-76.2169]), tensor([-76.1127]), tensor([-76.2147]), tensor([-76.0551]), tensor([-76.2388]), tensor([-75.9564]), tensor([-75.7584]), tensor([-75.7297]), tensor([-76.2609]), tensor([-76.3086]), tensor([-76.3055]), tensor([-76.2523]), tensor([-76.3168]), tensor([-76.2941]), tensor([-76.2478]), tensor([-76.0894]), tensor([-74.9338]), tensor([-76.2317]), tensor([-76.3237]), tensor([-76.3100]), tensor([-76.2765]), tensor([-76.2315]), tensor([-76.2133]), tensor([-76.3104]), tensor([-76.2975]), tensor([-75.9545]), tensor([-76.2457]), tensor([-76.2551]), tensor([-76.2162]), tensor([-76.1562]), tensor([-76.3010]), tensor([-76.3211]), tensor([-75.4061]), tensor([-76.3346]), tensor([-76.2601]), tensor([-76.2355]), tensor([-76.3277]), tensor([-76.2393]), tensor([-76.2690]), tensor([-76.3355]), tensor([-76.3482]), tensor([-76.2819]), tensor([-76.2348]), tensor([-76.2992]), tensor([-75.7831]), tensor([-75.9749]), tensor([-76.2454]), tensor([-76.2940]), tensor([-75.9059]), tensor([-76.2972]), tensor([-76.1863]), tensor([-76.2776]), tensor([-74.4173]), tensor([-76.3031]), tensor([-76.2626]), tensor([-76.2826]), tensor([-76.2565]), tensor([-76.2844]), tensor([-76.2769]), tensor([-76.2878]), tensor([-75.6375]), tensor([-76.1569]), tensor([-76.1722]), tensor([-76.2131]), tensor([-76.0619]), tensor([-76.2478]), tensor([-76.2607]), tensor([-76.0080]), tensor([-76.2817]), tensor([-76.2112]), tensor([-76.2156]), tensor([-76.2869]), tensor([-76.0678]), tensor([-76.3456]), tensor([-76.2706]), tensor([-76.2423]), tensor([-76.0192]), tensor([-76.2490]), tensor([-76.2860]), tensor([-76.2416]), tensor([-76.3081]), tensor([-76.1057]), tensor([-76.2951]), tensor([-76.2832]), tensor([-76.2741]), tensor([-76.2624]), tensor([-76.2474]), tensor([-76.1300]), tensor([-75.8520]), tensor([-76.2040]), tensor([-76.1983]), tensor([-76.2624]), tensor([-76.2420]), tensor([-76.2068]), tensor([-75.6385]), tensor([-76.2989]), tensor([-76.1098]), tensor([-76.3017]), tensor([-76.0770]), tensor([-76.2239]), tensor([-76.3198]), tensor([-76.2967]), tensor([-76.2664]), tensor([-76.1787]), tensor([-76.2034]), tensor([-76.3400]), tensor([-75.8935]), tensor([-76.1898]), tensor([-75.7312]), tensor([-76.2023]), tensor([-76.2609]), tensor([-76.2491]), tensor([-76.0189]), tensor([-75.9904]), tensor([-76.2254]), tensor([-74.6693]), tensor([-76.3335]), tensor([-76.1192]), tensor([-76.1413]), tensor([-75.0048]), tensor([-76.2260]), tensor([-76.2159]), tensor([-76.2594]), tensor([-75.9811]), tensor([-76.3406]), tensor([-76.1902]), tensor([-76.3471]), tensor([-75.8776]), tensor([-76.2421]), tensor([-76.2942]), tensor([-76.1754]), tensor([-76.0158]), tensor([-76.0732]), tensor([-76.3330]), tensor([-76.1153]), tensor([-76.2781]), tensor([-75.9347]), tensor([-76.2823]), tensor([-76.2537]), tensor([-76.3021]), tensor([-76.2476]), tensor([-76.2137]), tensor([-76.2461]), tensor([-76.0503]), tensor([-75.9403]), tensor([-75.9489]), tensor([-76.2875]), tensor([-76.2869]), tensor([-76.2342]), tensor([-76.2175]), tensor([-76.2321]), tensor([-76.2583]), tensor([-75.2289]), tensor([-76.2269]), tensor([-75.9258]), tensor([-76.2357]), tensor([-76.0513]), tensor([-76.3098]), tensor([-76.2811]), tensor([-76.3277]), tensor([-76.2677]), tensor([-76.2381]), tensor([-76.2217]), tensor([-76.3088]), tensor([-76.2347]), tensor([-76.2528]), tensor([-76.2549]), tensor([-76.2580]), tensor([-76.2181]), tensor([-76.3228]), tensor([-76.0077]), tensor([-76.1384]), tensor([-76.3022]), tensor([-76.2705]), tensor([-76.2651]), tensor([-76.3020]), tensor([-76.3208]), tensor([-75.2722]), tensor([-76.2586]), tensor([-76.3507]), tensor([-76.2841]), tensor([-76.3101]), tensor([-76.2156]), tensor([-76.3279]), tensor([-76.2059]), tensor([-76.1714]), tensor([-76.2410]), tensor([-75.7314]), tensor([-76.3108]), tensor([-76.1892]), tensor([-76.2463]), tensor([-76.0189]), tensor([-76.1049]), tensor([-76.3182]), tensor([-75.4806]), tensor([-76.2917]), tensor([-76.2530]), tensor([-76.3113]), tensor([-76.2823]), tensor([-76.2942]), tensor([-76.2043]), tensor([-76.2561]), tensor([-76.3186]), tensor([-76.2196]), tensor([-76.3249]), tensor([-76.2536]), tensor([-76.0877]), tensor([-76.3410]), tensor([-76.2657]), tensor([-76.1906]), tensor([-76.2149]), tensor([-76.2580]), tensor([-76.3089]), tensor([-76.1408]), tensor([-76.1392]), tensor([-75.9521]), tensor([-76.2288]), tensor([-76.2330]), tensor([-76.2247]), tensor([-76.0618]), tensor([-76.2716]), tensor([-76.2280]), tensor([-76.2961]), tensor([-76.1571]), tensor([-76.2394]), tensor([-76.2438]), tensor([-75.9911]), tensor([-76.2810]), tensor([-76.3266]), tensor([-76.2744]), tensor([-75.9846]), tensor([-76.1397]), tensor([-75.5905]), tensor([-75.6070]), tensor([-76.3078]), tensor([-76.2161]), tensor([-76.2582]), tensor([-76.2759]), tensor([-76.2347]), tensor([-76.2049]), tensor([-76.3468]), tensor([-76.2742]), tensor([-76.0742]), tensor([-76.2338]), tensor([-75.7742]), tensor([-76.2535]), tensor([-76.0526]), tensor([-76.3331]), tensor([-76.2644]), tensor([-75.9437]), tensor([-76.2376]), tensor([-75.9821]), tensor([-76.2461]), tensor([-75.6200]), tensor([-76.2631]), tensor([-75.6696]), tensor([-76.2563]), tensor([-76.3093]), tensor([-75.4026]), tensor([-76.2549]), tensor([-76.1871]), tensor([-75.6863]), tensor([-76.3022]), tensor([-76.2238]), tensor([-76.2400]), tensor([-75.9874]), tensor([-76.3099])]\n",
      "yhats:  [tensor([-76.1086], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2257], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8405], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2645], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2566], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2828], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2925], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1736], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2646], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2567], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2335], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2754], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2996], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2722], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2981], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2832], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2464], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2649], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2076], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2489], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2346], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2552], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1082], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.4632], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5568], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2546], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7685], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9642], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2473], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1346], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2259], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2406], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1335], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0587], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1330], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2912], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2586], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2551], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2738], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2618], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2648], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2470], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1977], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2544], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2447], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1772], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2443], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2247], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9400], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2210], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2885], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1596], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2782], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2284], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2932], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2444], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2821], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2450], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2712], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2395], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1708], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2566], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2766], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0001], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2613], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2211], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2420], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2263], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2524], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2459], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2372], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2745], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2358], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2519], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1682], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2337], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9311], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2257], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2673], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1419], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2456], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2730], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2691], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.3505], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2968], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2135], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2423], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2341], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2547], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2638], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2727], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8687], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2703], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1589], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0873], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1546], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9873], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2450], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9496], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7560], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.6428], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2583], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2643], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2942], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2462], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2832], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2615], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2571], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0657], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-74.9618], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2458], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2733], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2888], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2573], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2380], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2452], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2660], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2769], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9379], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2395], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2458], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2238], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1407], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2586], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2806], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.3510], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2840], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2269], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2314], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2864], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2563], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2686], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2888], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2981], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2600], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2315], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2710], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7411], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9394], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2210], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2607], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9051], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2444], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2089], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2633], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-74.3771], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2627], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2461], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2835], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2107], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2620], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2480], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2783], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5669], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1456], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1624], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2324], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0457], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2388], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2483], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9421], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2300], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2007], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2407], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2354], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0469], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2970], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2337], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2432], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9850], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2529], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2521], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2439], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2955], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0380], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2610], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2620], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2243], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2516], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2332], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0905], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7882], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2243], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1511], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2476], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2336], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2303], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5687], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2772], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0756], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2644], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9826], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1838], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2761], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2620], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2482], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1888], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2455], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.3015], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8684], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1658], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7442], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2247], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2479], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2604], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0107], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9650], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2514], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-74.6323], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2821], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1233], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0959], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-74.9787], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2354], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2095], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2248], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9133], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2950], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1769], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2938], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8196], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2581], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2667], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1647], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9909], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0246], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2848], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0669], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2440], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8424], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2530], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2458], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2512], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2245], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2437], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2451], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9662], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8893], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9009], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2674], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2519], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2458], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2402], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2293], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2380], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.2292], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2439], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8927], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2452], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0389], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2694], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2581], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2961], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2689], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2409], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1937], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2653], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2404], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2393], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2397], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2088], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2147], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2896], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9316], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1351], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2576], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2645], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2620], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2789], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2749], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.2558], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2529], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2954], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2764], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2658], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2387], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2907], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2336], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1354], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2456], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.6345], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2708], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1583], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2436], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9538], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0522], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2780], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.4297], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2742], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2430], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2692], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2552], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2792], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2319], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2486], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2755], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1984], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2981], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2587], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0972], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2884], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2385], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1966], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2112], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2451], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2705], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1311], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0826], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8543], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2219], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2074], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2274], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9978], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2487], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2357], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2657], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0980], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1850], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2338], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9767], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2533], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2861], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2528], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9019], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1240], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5852], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.6016], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2634], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2002], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2430], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2488], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2495], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1745], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2982], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2476], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.0185], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2452], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.7104], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2598], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9947], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2831], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2531], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8184], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2402], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.9473], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2390], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5974], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2434], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.5395], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2123], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2640], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.3440], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2414], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1526], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.6042], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2787], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2448], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.1803], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-75.8996], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([-76.2660], dtype=torch.float64, grad_fn=<AddBackward0>)]\n",
      "MSE: 0.0013904362912760667\n",
      "RMSE: 0.03728855442727791\n",
      "R-squared (R2) Score: 0.9755871667882676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python3.9.2\\lib\\site-packages\\numpy\\core\\shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n",
      "c:\\Python\\Python3.9.2\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted Energy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "ys = []\n",
    "yhats = []\n",
    "for v in validation_set:\n",
    "    (e, x), y = convert_record(v)\n",
    "    ys.append(y)\n",
    "    # yhat_raw = Model(e, x, w1, w2, w3, b)\n",
    "    model = Model(w1, w2, w3, b, graph_feature_len)\n",
    "    model.eval()\n",
    "    yhat_raw = model(e, x)\n",
    "    \n",
    "    yhats.append(transform_prediction(yhat_raw))\n",
    "\n",
    "\n",
    "yhats1 = []\n",
    "for i in yhats:\n",
    "    i = i.detach().numpy()\n",
    "    yhats1.append(i)\n",
    "    \n",
    "ys_new  = []\n",
    "\n",
    "for i in ys:\n",
    "    i = i.detach().numpy()\n",
    "    ys_new.append(i)\n",
    "    \n",
    "print(\"ys: \", ys)\n",
    "print(\"yhats: \", yhats)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(ys_new, yhats1)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "r2 = r2_score(ys_new, yhats1)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "\n",
    "plt.plot(ys, ys, \"-\")\n",
    "plt.plot(ys, yhats1, \".\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Predicted Energy\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
