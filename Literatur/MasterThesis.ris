TY  - JOUR
TI  - Analyzing the Past to Prepare for the Future: Writing a Literature Review
AU  - Webster, Jane
AU  - Watson, Richard T.
T2  - MIS Quarterly
DA  - 2002///
PY  - 2002
DP  - Zotero
VL  - 26
IS  - 2
SP  - xiii
EP  - xxiii
LA  - en
UR  - http://www.jstor.org/stable/4132319
L1  - http://ww.gossettphd.org/library/sciwriting/webster_howto-litreview%20(1).pdf
ER  - 

TY  - JOUR
TI  - State-of-the-Art des State-of-the-Art: Eine Untersuchung der Forschungsmethode „Review” innerhalb der Wirtschaftsinformatik
AU  - Fettke, Peter
T2  - WIRTSCHAFTSINFORMATIK
DA  - 2006/08//
PY  - 2006
DO  - 10.1007/s11576-006-0057-3
DP  - DOI.org (Crossref)
VL  - 48
IS  - 4
SP  - 257
J2  - Wirtsch. Inform.
LA  - de
SN  - 0937-6429, 1861-8936
ST  - State-of-the-Art des State-of-the-Art
UR  - http://link.springer.com/10.1007/s11576-006-0057-3
Y2  - 2022/10/15/14:58:36
L1  - https://link.springer.com/content/pdf/10.1007/s11576-006-0057-3.pdf
ER  - 

TY  - JOUR
TI  - RECONSTRUCTING THE GIANT: ON THE IMPORTANCE OF RIGOUR IN DOCUMENTING THE LITERATURE SEARCH PROCESS
AU  - vom Brocke, Jan
AU  - Simons, Alexander
AU  - Niehaves, Bjoern
AU  - Reimer, Kai
AB  - Science is a cumulative endeavour as new knowledge is often created in the process of interpreting and combining existing knowledge. This is why literature reviews have long played a decisive role in scholarship. The quality of literature reviews is particularly determined by the literature search process. As Sir Isaac Newton eminently put it: “If I can see further, it is because I am standing on the shoulders of giants.” Drawing on this metaphor, the goal of writing a literature review is to reconstruct the giant of accumulated knowledge in a specific domain. And in doing so, a literature search represents the fundamental first step that makes up the giant’s skeleton and largely determines its reconstruction in the subsequent literature analysis. In this paper, we argue that the process of searching the literature must be comprehensibly described. Only then can readers assess the exhaustiveness of the review and other scholars in the field can more confidently (re)use the results in their own research. We set out to explore the methodological rigour of literature review articles published in ten major information systems (IS) journals and show that many of these reviews do not thoroughly document the process of literature search. The results drawn from our analysis lead us to call for more rigour in documenting the literature search process and to present guidelines for crafting a literature review and search in the IS domain.
DP  - Zotero
LA  - en
L1  - https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1145&context=ecis2009
ER  - 

TY  - JOUR
TI  - Hydrogen production through renewable and non-renewable energy processes and their impact on climate change
AU  - Amin, Muhammad
AU  - Shah, Hamad Hussain
AU  - Fareed, Anaiz Gul
AU  - Khan, Wasim Ullah
AU  - Chung, Eunhyea
AU  - Zia, Adeel
AU  - Rahman Farooqi, Zia Ur
AU  - Lee, Chaehyeon
T2  - International Journal of Hydrogen Energy
AB  - The urbanization and increase in the human population has significantly influenced the global energy demands. The utilization of non-renewable fossil fuel-based energy infrastructure involves air pollution, global warming due to CO2 emissions, greenhouse gas emissions, acid rains, diminishing energy resources, and environmental degradation leading to climate change due to global warming. These factors demand the exploration of alternative energy sources based on renewable sources. Hydrogen, an efficient energy carrier, has emerged as an alternative fuel to meet energy demands and green hydrogen production with zero carbon emission has gained scientific attraction in recent years. This review is focused on the production of hydrogen from renewable sources such as biomass, solar, wind, geothermal, and algae and conventional non-renewable sources including natural gas, coal, nuclear and thermochemical processes. Moreover, the cost analysis for hydrogen production from each source of energy is discussed. Finally, the impact of these hydrogen production processes on the environment and their implications are summarized.
DA  - 2022/09/08/
PY  - 2022
DO  - 10.1016/j.ijhydene.2022.07.172
DP  - ScienceDirect
VL  - 47
IS  - 77
SP  - 33112
EP  - 33134
J2  - International Journal of Hydrogen Energy
SN  - 0360-3199
UR  - https://www.sciencedirect.com/science/article/pii/S0360319922032244
Y2  - 2023/09/12/07:20:07
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0360319922032244
KW  - Cost
KW  - Environment
KW  - Hydrogen
KW  - Non-renewable
KW  - Renewable
ER  - 

TY  - JOUR
TI  - Research frontiers in sustainable development of energy, water and environment systems in a time of climate crisis
AU  - Kılkış, Şiir
AU  - Krajačić, Goran
AU  - Duić, Neven
AU  - Montorsi, Luca
AU  - Wang, Qiuwang
AU  - Rosen, Marc A.
AU  - Ahmad Al-Nimr, Moh'd
T2  - Energy Conversion and Management
AB  - Sustainable energy conversion and management processes increasingly require an integrated approach, especially in the context of addressing the climate crisis. This editorial puts forth related research frontiers based on 28 research articles of the special issue that is dedicated to the 13th Conference on Sustainable Development of Energy, Water and Environment Systems and regional series based on the 1st Latin American and 3rd South East European Conferences. Seven research frontiers are reviewed, the first three of which are (i) sustainable technologies for local energy systems, (ii) energy storage and advances in flexibility and (iii) solar energy penetration across multiple sectors. These research frontiers contain contributions based on renewable energy for wastewater treatment in islands, energy savings across urban built infrastructure, advanced district heating and cooling networks, power-to-gas and hydrogen production technologies, demand response in industrial systems, hybrid thermal energy storage, hybrid solar energy power plants, novel photovoltaic thermal technologies, and improved solar energy dispatchability. The research frontiers continue with (iv) wind, water based energy and the energy-water nexus, (v) effective valorization and upgrading of resources, (vi) combustion processes and better utilization of heat and (vii) carbon capture, storage and utilization. Significant contributions include innovative wind and hydrokinetic turbines, osmotic power technologies, synergetic solutions for water desalination, efficient catalytic pyrolysis, upgrading to reduce particle pollution, co-processing for alternative fuels, combustion characterization, electricity generation from waste heat sources, advances in heat exchangers and heat transfer, oxy-fuel combustion, post-combustion capture, and fly ash recycling for energy storage material. The research frontiers in this editorial provide ample opportunities to support societal transformations in the next decades to sustain planetary life-support systems.
DA  - 2019/11/01/
PY  - 2019
DO  - 10.1016/j.enconman.2019.111938
DP  - ScienceDirect
VL  - 199
SP  - 111938
J2  - Energy Conversion and Management
SN  - 0196-8904
UR  - https://www.sciencedirect.com/science/article/pii/S019689041930929X
Y2  - 2023/09/12/07:31:27
L2  - https://www.sciencedirect.com/science/article/abs/pii/S019689041930929X?via%3Dihub
KW  - Environment
KW  - Climate mitigation
KW  - Energy
KW  - Sustainable development
KW  - System integration
KW  - Water
ER  - 

TY  - JOUR
TI  - Innovative Electrochemical Strategies for Hydrogen Production: From Electricity Input to Electricity Output
AU  - Yan, Dafeng
AU  - Mebrahtu, Chalachew
AU  - Wang, Shuangyin
AU  - Palkovits, Regina
T2  - Angewandte Chemie International Edition
AB  - Renewable H2 production by water electrolysis has attracted much attention due to its numerous advantages. However, the energy consumption of conventional water electrolysis is high and mainly driven by the kinetically inert anodic oxygen evolution reaction. An alternative approach is the coupling of different half-cell reactions and the use of redox mediators. In this review, we, therefore, summarize the latest findings on innovative electrochemical strategies for H2 production. First, we address redox mediators utilized in water splitting, including soluble and insoluble species, and the corresponding cell concepts. Second, we discuss alternative anodic reactions involving organic and inorganic chemical transformations. Then, electrochemical H2 production at both the cathode and anode, or even H2 production together with electricity generation, is presented. Finally, the remaining challenges and prospects for the future development of this research field are highlighted.
DA  - 2023/04/11/
PY  - 2023
DO  - 10.1002/anie.202214333
DP  - DOI.org (Crossref)
VL  - 62
IS  - 16
SP  - e202214333
J2  - Angew Chem Int Ed
LA  - en
SN  - 1433-7851, 1521-3773
ST  - Innovative Electrochemical Strategies for Hydrogen Production
UR  - https://onlinelibrary.wiley.com/doi/10.1002/anie.202214333
Y2  - 2023/09/12/07:41:48
L1  - https://publications.rwth-aachen.de/record/963285/files/963285.pdf
ER  - 

TY  - JOUR
TI  - Waste-Derived Catalysts for Water Electrolysis: Circular Economy-Driven Sustainable Green Hydrogen Energy
AU  - Chen, Zhijie
AU  - Yun, Sining
AU  - Wu, Lan
AU  - Zhang, Jiaqi
AU  - Shi, Xingdong
AU  - Wei, Wei
AU  - Liu, Yiwen
AU  - Zheng, Renji
AU  - Han, Ning
AU  - Ni, Bing-Jie
T2  - Nano-Micro Letters
AB  - The sustainable production of green hydrogen via water electrolysis necessitates cost-effective electrocatalysts. By following the circular economy principle, the utilization of waste-derived catalysts sig‑nificantly promotes the sustainable development of green hydrogen energy. Currently, diverse waste-derived catalysts have exhibited excellent catalytic performance toward hydrogen evolution reaction (HER), oxygen evolution reaction (OER), and overall water electrolysis (OWE). Herein, we system‑atically examine recent achievements in waste-derived electrocatalysts for water electrolysis. The general principles of water electrolysis and design principles of efficient electrocatalysts are discussed, followed by the illustra‑tion of current strategies for transforming wastes into electrocatalysts. Then, applications of waste-derived catalysts (i.e., carbon-based catalysts, transi‑tional metal-based catalysts, and carbon-based heterostructure catalysts) in HER, OER, and OWE are reviewed successively. An emphasis is put on cor‑relating the catalysts’ structure–performance relationship. Also, challenges and research directions in this booming field are finally highlighted. This review would provide useful insights into the design, synthesis, and applications of waste-derived electrocatalysts, and thus accelerate the development of the circular economy-driven green hydrogen energy scheme.
DA  - 2023/12//
PY  - 2023
DO  - 10.1007/s40820-022-00974-7
DP  - DOI.org (Crossref)
VL  - 15
IS  - 1
SP  - 4
J2  - Nano-Micro Lett.
LA  - en
SN  - 2311-6706, 2150-5551
ST  - Waste-Derived Catalysts for Water Electrolysis
UR  - https://link.springer.com/10.1007/s40820-022-00974-7
Y2  - 2023/09/12/07:51:48
ER  - 

TY  - JOUR
TI  - The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts
AU  - Tran, Richard
AU  - Lan, Janice
AU  - Shuaibi, Muhammed
AU  - Wood, Brandon M.
AU  - Goyal, Siddharth
AU  - Das, Abhishek
AU  - Heras-Domingo, Javier
AU  - Kolluru, Adeesh
AU  - Rizvi, Ammar
AU  - Shoghi, Nima
AU  - Sriram, Anuroop
AU  - Therrien, Felix
AU  - Abed, Jehad
AU  - Voznyy, Oleksandr
AU  - Sargent, Edward H.
AU  - Ulissi, Zachary
AU  - Zitnick, C. Lawrence
T2  - ACS Catalysis
AB  - The development of machine learning models for electrocatalysts requires a broad set of training data to enable their use across a wide variety of materials. One class of materials that currently lacks sufficient training data is oxides, which are critical for the development of OER catalysts. To address this, we developed the OC22 dataset, consisting of 62,331 DFT relaxations (~9,854,504 single point calculations) across a range of oxide materials, coverages, and adsorbates. We define generalized total energy tasks that enable property prediction beyond adsorption energies; we test baseline performance of several graph neural networks; and we provide pre-defined dataset splits to establish clear benchmarks for future efforts. In the most general task, GemNet-OC sees a ~36% improvement in energy predictions when combining the chemically dissimilar OC20 and OC22 datasets via fine-tuning. Similarly, we achieved a ~19% improvement in total energy predictions on OC20 and a ~9% improvement in force predictions in OC22 when using joint training. We demonstrate the practical utility of a top performing model by capturing literature adsorption energies and important OER scaling relationships. We expect OC22 to provide an important benchmark for models seeking to incorporate intricate long-range electrostatic and magnetic interactions in oxide surfaces. Dataset and baseline models are open sourced, and a public leaderboard is available to encourage continued community developments on the total energy tasks and data.
DA  - 2023/03/03/
PY  - 2023
DO  - 10.1021/acscatal.2c05426
DP  - arXiv.org
VL  - 13
IS  - 5
SP  - 3066
EP  - 3084
J2  - ACS Catal.
SN  - 2155-5435, 2155-5435
UR  - http://arxiv.org/abs/2206.08917
Y2  - 2023/09/12/07:54:44
L1  - https://arxiv.org/pdf/2206.08917.pdf
L2  - https://arxiv.org/abs/2206.08917
KW  - Computer Science - Machine Learning
KW  - Condensed Matter - Materials Science
KW  - Physics - Computational Physics
ER  - 

TY  - GEN
TI  - Quantum machine learning of graph-structured data
AU  - Beer, Kerstin
AU  - Khosla, Megha
AU  - Köhler, Julius
AU  - Osborne, Tobias J.
AB  - Graph structures are ubiquitous throughout the natural sciences. Here we consider graph-structured quantum data and describe how to carry out its quantum machine learning via quantum neural networks. In particular, we consider training data in the form of pairs of input and output quantum states associated with the vertices of a graph, together with edges encoding correlations between the vertices. We explain how to systematically exploit this additional graph structure to improve quantum learning algorithms. These algorithms are numerically simulated and exhibit excellent learning behavior. Scalable quantum implementations of the learning procedures are likely feasible on the next generation of quantum computing devices.
DA  - 2021/03/19/
PY  - 2021
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2103.10837
Y2  - 2023/09/07/07:57:18
L1  - https://arxiv.org/pdf/2103.10837.pdf
L2  - https://arxiv.org/abs/2103.10837
KW  - Quantum Physics
ER  - 

TY  - JOUR
TI  - A Design Science Research Methodology for Information Systems Research
AU  - Peffers, Ken
AU  - Tuunanen, Tuure
AU  - Rothenberger, Marcus A.
AU  - Chatterjee, Samir
T2  - Journal of Management Information Systems
AB  - The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.
DA  - 2007/12/01/
PY  - 2007
DO  - 10.2753/MIS0742-1222240302
DP  - Taylor and Francis+NEJM
VL  - 24
IS  - 3
SP  - 45
EP  - 77
SN  - 0742-1222
UR  - https://doi.org/10.2753/MIS0742-1222240302
Y2  - 2023/09/12/08:56:55
KW  - case study
KW  - design science
KW  - design science research
KW  - design theory
KW  - mental model
KW  - methodology
KW  - process model
ER  - 

TY  - JOUR
TI  - A Comprehensive Survey on Graph Neural Networks
AU  - Wu, Zonghan
AU  - Pan, Shirui
AU  - Chen, Fengwen
AU  - Long, Guodong
AU  - Zhang, Chengqi
AU  - Yu, Philip S.
T2  - IEEE Transactions on Neural Networks and Learning Systems
AB  - Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classiﬁcation and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed signiﬁcant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning ﬁelds. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing ﬁeld.
DA  - 2021/01//
PY  - 2021
DO  - 10.1109/TNNLS.2020.2978386
DP  - DOI.org (Crossref)
VL  - 32
IS  - 1
SP  - 4
EP  - 24
J2  - IEEE Trans. Neural Netw. Learning Syst.
LA  - en
SN  - 2162-237X, 2162-2388
UR  - https://ieeexplore.ieee.org/document/9046288/
Y2  - 2023/11/21/14:56:05
L1  - https://ieeexplore.ieee.org/ielaam/5962385/9312808/9046288-aam.pdf
ER  - 

TY  - JOUR
TI  - Graph neural networks: A review of methods and applications
AU  - Zhou, Jie
AU  - Cui, Ganqu
AU  - Hu, Shengding
AU  - Zhang, Zhengyan
AU  - Yang, Cheng
AU  - Liu, Zhiyuan
AU  - Wang, Lifeng
AU  - Li, Changcheng
AU  - Sun, Maosong
T2  - AI Open
AB  - Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.
DA  - 2020///
PY  - 2020
DO  - 10.1016/j.aiopen.2021.01.001
DP  - DOI.org (Crossref)
VL  - 1
SP  - 57
EP  - 81
J2  - AI Open
LA  - en
SN  - 26666510
ST  - Graph neural networks
UR  - https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012
Y2  - 2023/11/21/15:01:11
ER  - 

TY  - JOUR
TI  - Computational Prediction of Graphdiyne-Supported Three-Atom Single-Cluster Catalysts
AU  - Liu, Jin-Cheng
AU  - Xiao, Hai
AU  - Zhao, Xiao-Kun
AU  - Zhang, Nan-Nan
AU  - Liu, Yuan
AU  - Xing, Deng-Hui
AU  - Yu, Xiaohu
AU  - Hu, Han-Shi
AU  - Li, Jun
T2  - CCS Chemistry
DA  - 2023/01/10/
PY  - 2023
DO  - 10.31635/ccschem.022.202201796
DP  - DOI.org (Crossref)
VL  - 5
IS  - 1
SP  - 152
EP  - 163
J2  - CCS Chem
LA  - en
SN  - 2096-5745
UR  - http://www.chinesechemsoc.org/doi/10.31635/ccschem.022.202201796
Y2  - 2023/11/21/15:33:20
L1  - https://www.chinesechemsoc.org/doi/pdf/10.31635/ccschem.022.202201796
ER  - 

TY  - JOUR
TI  - Everything is Connected: Graph Neural Networks
AU  - Veličković, Petar
T2  - Current Opinion in Structural Biology
AB  - In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.
DA  - 2023/04//
PY  - 2023
DO  - 10.1016/j.sbi.2023.102538
DP  - arXiv.org
VL  - 79
SP  - 102538
J2  - Current Opinion in Structural Biology
SN  - 0959440X
ST  - Everything is Connected
UR  - http://arxiv.org/abs/2301.08210
Y2  - 2023/11/21/15:46:01
L1  - https://arxiv.org/pdf/2301.08210.pdf
L2  - https://arxiv.org/abs/2301.08210
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Social and Information Networks
ER  - 

TY  - GEN
TI  - How Powerful are Graph Neural Networks?
AU  - Xu, Keyulu
AU  - Hu, Weihua
AU  - Leskovec, Jure
AU  - Jegelka, Stefanie
AB  - Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.
DA  - 2019/02/22/
PY  - 2019
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1810.00826
Y2  - 2023/11/21/16:06:10
L1  - https://arxiv.org/pdf/1810.00826.pdf
L2  - https://arxiv.org/abs/1810.00826
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - GEN
TI  - Directional Message Passing for Molecular Graphs
AU  - Gasteiger, Johannes
AU  - Groß, Janek
AU  - Günnemann, Stephan
AB  - Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9. Our implementation is available online.
DA  - 2022/04/05/
PY  - 2022
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/2003.03123
Y2  - 2023/11/21/16:14:36
L1  - https://arxiv.org/pdf/2003.03123.pdf
L2  - https://arxiv.org/abs/2003.03123
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
KW  - Physics - Computational Physics
ER  - 

TY  - ELEC
TI  - torch_geometric.datasets.QM9 — pytorch_geometric documentation
UR  - https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.QM9.html
Y2  - 2023/11/22/12:40:07
L2  - https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.QM9.html
ER  - 

TY  - JOUR
TI  - CRISP-DM: Towards a Standard Process Model for Data Mining
AU  - Wirth, Rüdiger
AU  - Hipp, Jochen
AB  - The CRISP-DM (CRoss Industry Standard Process for Data Mining) project proposed a comprehensive process model for carrying out data mining projects. The process model is independent of both the industry sector and the technology used. In this paper we argue in favor of a standard process model for data mining and report some experiences with the CRISP-DM process model in practice.
DP  - Zotero
LA  - en
L1  - https://www.cs.unibo.it/~danilo.montesi/CBD/Beatriz/10.1.1.198.5133.pdf
ER  - 

TY  - CONF
TI  - Neural Message Passing for Quantum Chemistry
AU  - Gilmer, Justin
AU  - Schoenholz, Samuel S.
AU  - Riley, Patrick F.
AU  - Vinyals, Oriol
AU  - Dahl, George E.
T2  - International Conference on Machine Learning
AB  - Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.
C3  - Proceedings of the 34th International Conference on Machine Learning
DA  - 2017/07/17/
PY  - 2017
DP  - proceedings.mlr.press
SP  - 1263
EP  - 1272
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v70/gilmer17a.html
Y2  - 2023/11/27/11:14:32
L1  - https://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf
ER  - 

TY  - ELEC
TI  - DeepChem
UR  - https://deepchem.io/about/
Y2  - 2023/11/27/11:18:55
L2  - https://deepchem.io/about/
ER  - 

TY  - JOUR
TI  - Low Data Drug Discovery with One-Shot Learning
AU  - Altae-Tran, Han
AU  - Ramsundar, Bharath
AU  - Pappu, Aneesh S.
AU  - Pande, Vijay
T2  - ACS Central Science
DA  - 2017/04/26/
PY  - 2017
DO  - 10.1021/acscentsci.6b00367
DP  - DOI.org (Crossref)
VL  - 3
IS  - 4
SP  - 283
EP  - 293
J2  - ACS Cent. Sci.
LA  - en
SN  - 2374-7943, 2374-7951
UR  - https://pubs.acs.org/doi/10.1021/acscentsci.6b00367
Y2  - 2023/11/27/11:35:54
L1  - https://pubs.acs.org/doi/pdf/10.1021/acscentsci.6b00367
ER  - 

TY  - JOUR
TI  - Pushing the boundaries of molecular representation for drug discovery with graph attention mechanism
AU  - Xiong, Zhaoping
AU  - Wang, Dingyan
AU  - Liu, Xiaohong
AU  - Feisheng, Zhong
AU  - Wan, Xiaozhe
AU  - Li, Xutong
AU  - Li, Zhaojun
AU  - Luo, Xiaomin
AU  - Chen, Kaixian
AU  - Jiang, H.
AU  - Zheng, Mingyue
T2  - Journal of Medicinal Chemistry
AB  - Hunting for chemicals with favourable pharmacological, toxicological and pharmacokinetic properties remains a formidable challenge for drug discovery. Deep learning provides us with powerful tools to build predictive models that are appropriate for the rising amounts of data, but the gap between what these neural networks learn and what human beings can comprehend is growing. Moreover, this gap may induce distrust and restrict deep learning applications in practice. Here, we introduce a new graph neural network architecture called Attentive FP for molecular representation that uses a graph attention mechanism to learn from relevant drug discovery datasets. We demonstrate that Attentive FP achieves state-of-the-art predictive performances on a variety of datasets and that what it learns is interpretable. The feature visualization for Attentive FP suggests that it automatically learns non-local intramolecular interactions from specified tasks, which can help us gain chemical insights directly from data beyond human perception.
DA  - 2019/08/13/
PY  - 2019
DO  - 10.1021/acs.jmedchem.9b00959
DP  - ResearchGate
VL  - 63
J2  - Journal of Medicinal Chemistry
L1  - https://www.researchgate.net/profile/Zhaoping-Xiong/publication/335156258_Pushing_the_boundaries_of_molecular_representation_for_drug_discovery_with_graph_attention_mechanism/links/5e0aac65a6fdcc28374addc0/Pushing-the-boundaries-of-molecular-representation-for-drug-discovery-with-graph-attention-mechanism.pdf
L4  - https://www.researchgate.net/profile/Zhaoping-Xiong/publication/335156258_Pushing_the_boundaries_of_molecular_representation_for_drug_discovery_with_graph_attention_mechanism/links/5e0aac65a6fdcc28374addc0/Pushing-the-boundaries-of-molecular-representation-for-drug-discovery-with-graph-attention-mechanism.pdf
ER  - 

TY  - JOUR
TI  - Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models
AU  - Jiang, Dejun
AU  - Wu, Zhenxing
AU  - Hsieh, Chang-Yu
AU  - Chen, Guangyong
AU  - Liao, Ben
AU  - Wang, Zhe
AU  - Shen, Chao
AU  - Cao, Dongsheng
AU  - Wu, Jian
AU  - Hou, Tingjun
T2  - Journal of Cheminformatics
AB  - Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property predic‑tion, and numerous studies have shown that GNN could yield more promising results than traditional descriptorbased methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algo‑rithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reli‑able predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and dem‑onstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptorbased models still can be directly employed to accurately predict various chemical endpoints with excellent comput‑ability and interpretability.
DA  - 2021/02/17/
PY  - 2021
DO  - 10.1186/s13321-020-00479-8
DP  - DOI.org (Crossref)
VL  - 13
IS  - 1
SP  - 12
J2  - J Cheminform
LA  - en
SN  - 1758-2946
ST  - Could graph neural networks learn better molecular representation for drug discovery?
UR  - https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00479-8
Y2  - 2023/11/27/12:11:16
ER  - 

TY  - GEN
TI  - Semi-Supervised Classification with Graph Convolutional Networks
AU  - Kipf, Thomas N.
AU  - Welling, Max
AB  - We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.
DA  - 2017/02/22/
PY  - 2017
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1609.02907
Y2  - 2023/11/27/12:32:20
L1  - https://arxiv.org/pdf/1609.02907.pdf%EF%BC%89
KW  - Computer Science - Machine Learning
KW  - Statistics - Machine Learning
ER  - 

TY  - JOUR
TI  - Graph convolutional networks: a comprehensive review
AU  - Zhang, Si
AU  - Tong, Hanghang
AU  - Xu, Jiejun
AU  - Maciejewski, Ross
T2  - Computational Social Networks
AB  - Graphs naturally appear in numerous application domains, ranging from social analysis, bioinformatics to computer vision. The unique capability of graphs enables capturing the structural relations among data, and thus allows to harvest more insights compared to analyzing data in isolation. However, it is often very challenging to solve the learning problems on graphs, because (1) many types of data are not originally structured as graphs, such as images and text data, and (2) for graph-structured data, the underlying connectivity patterns are often complex and diverse. On the other hand, the representation learning has achieved great successes in many areas. Thereby, a potential solution is to learn the representation of graphs in a low-dimensional Euclidean space, such that the graph properties can be preserved. Although tremendous efforts have been made to address the graph representation learning problem, many of them still suffer from their shallow learning mechanisms. Deep learning models on graphs (e.g., graph neural networks) have recently emerged in machine learning and other related areas, and demonstrated the superior performance in various problems. In this survey, despite numerous types of graph neural networks, we conduct a comprehensive review specifically on the emerging field of graph convolutional networks, which is one of the most prominent graph deep learning models. First, we group the existing graph convolutional network models into two categories based on the types of convolutions and highlight some graph convolutional network models in details. Then, we categorize different graph convolutional networks according to the areas of their applications. Finally, we present several open challenges in this area and discuss potential directions for future research.
DA  - 2019/12//
PY  - 2019
DO  - 10.1186/s40649-019-0069-y
DP  - DOI.org (Crossref)
VL  - 6
IS  - 1
SP  - 11
J2  - Comput Soc Netw
LA  - en
SN  - 2197-4314
ST  - Graph convolutional networks
UR  - https://computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-019-0069-y
Y2  - 2023/11/27/12:47:17
ER  - 

TY  - ELEC
TI  - PyTorch
T2  - PyTorch
LA  - en
UR  - https://pytorch.org/
Y2  - 2023/11/28/12:03:26
L2  - https://pytorch.org/
ER  - 

TY  - GEN
TI  - Quantum Graph Convolutional Neural Networks
AU  - Zheng, Jin
AU  - Gao, Qing
AU  - Lv, Yanxuan
AB  - At present, there are a large number of quantum neural network models to deal with Euclidean spatial data, while little research have been conducted on non-Euclidean spatial data. In this paper, we propose a novel quantum graph convolutional neural network (QGCN) model based on quantum parametric circuits and utilize the computing power of quantum systems to accomplish graph classiﬁcation tasks in traditional machine learning. The proposed QGCN model has a similar architecture as the classical graph convolutional neural networks, which can illustrate the topology of the graph type data and efﬁciently learn the hidden layer representation of node features as well. Numerical simulation results on a graph dataset demonstrate that the proposed model can be effectively trained and has good performance in graph level classiﬁcation tasks.
DA  - 2021/07/07/
PY  - 2021
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/2107.03257
Y2  - 2024/01/27/12:56:56
KW  - Electrical Engineering and Systems Science - Signal Processing
ER  - 

TY  - GEN
TI  - Towards Quantum Graph Neural Networks: An Ego-Graph Learning Approach
AU  - Ai, Xing
AU  - Zhang, Zhihong
AU  - Sun, Luzhe
AU  - Yan, Junchi
AU  - Hancock, Edwin
AB  - Quantum machine learning is a fast-emerging ﬁeld that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This mapping maintains the distance relations present in the data and reduces information loss. Experimental results show that the proposed method outperforms competitive state-of-the-art models with only 1.68% parameters compared to those models.
DA  - 2024/01/19/
PY  - 2024
DP  - arXiv.org
LA  - en
PB  - arXiv
ST  - Towards Quantum Graph Neural Networks
UR  - http://arxiv.org/abs/2201.05158
Y2  - 2024/01/27/12:56:59
KW  - Computer Science - Machine Learning
KW  - Quantum Physics
ER  - 

TY  - GEN
TI  - Quantum Graph Neural Networks
AU  - Verdon, Guillaume
AU  - McCourt, Trevor
AU  - Luzhnica, Enxhell
AU  - Singh, Vikash
AU  - Leichenauer, Stefan
AU  - Hidary, Jack
AB  - We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classiﬁcation.
DA  - 2019/09/26/
PY  - 2019
DP  - arXiv.org
LA  - en
PB  - arXiv
UR  - http://arxiv.org/abs/1909.12264
Y2  - 2024/01/27/12:57:03
KW  - Computer Science - Machine Learning
KW  - Quantum Physics
ER  - 

TY  - JOUR
TI  - Quantum Machine Learning—An Overview
AU  - Tychola, Kyriaki A.
AU  - Kalampokas, Theofanis
AU  - Papakostas, George A.
T2  - Electronics
AB  - Quantum computing has been proven to excel in factorization issues and unordered search problems due to its capability of quantum parallelism. This unique feature allows exponential speedup in solving certain problems. However, this advantage does not apply universally, and challenges arise when combining classical and quantum computing to achieve acceleration in computation speed. This paper aims to address these challenges by exploring the current state of quantum machine learning and benchmarking the performance of quantum and classical algorithms in terms of accuracy. Speciﬁcally, we conducted experiments with three datasets for binary classiﬁcation, implementing Support Vector Machine (SVM) and Quantum SVM (QSVM) algorithms. Our ﬁndings suggest that the QSVM algorithm outperforms classical SVM on complex datasets, and the performance gap between quantum and classical models increases with dataset complexity, as simple models tend to overﬁt with complex datasets. While there is still a long way to go in terms of developing quantum hardware with sufﬁcient resources, quantum machine learning holds great potential in areas such as unsupervised learning and generative models. Moving forward, more efforts are needed to explore new quantum learning models that can leverage the power of quantum mechanics to overcome the limitations of classical machine learning.
DA  - 2023/05/24/
PY  - 2023
DO  - 10.3390/electronics12112379
DP  - DOI.org (Crossref)
VL  - 12
IS  - 11
SP  - 2379
J2  - Electronics
LA  - en
SN  - 2079-9292
UR  - https://www.mdpi.com/2079-9292/12/11/2379
Y2  - 2024/01/27/17:11:44
ER  - 

TY  - JOUR
TI  - Qualitative Content Analysis
AU  - Mayring, Philipp
AB  - The article describes an approach of systematic, rule guided qualitative text analysis, which tries to preserve some methodological strengths of quantitative content analysis and widen them to a concept of qualitative procedure.
DP  - Zotero
LA  - en
ER  - 

TY  - BOOK
TI  - Recommended Steps for Thematic Synthesis in Software Engineering
AU  - Cruzes, Daniela
AU  - Dybå, Tore
AB  - Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.
DA  - 2011/10/23/
PY  - 2011
DP  - ResearchGate
SP  - 275
L1  - https://www.researchgate.net/profile/Tore-Dyba/publication/224266207_Recommended_Steps_for_Thematic_Synthesis_in_Software_Engineering/links/00b7d5342845262b79000000/Recommended-Steps-for-Thematic-Synthesis-in-Software-Engineering.pdf
L4  - https://www.researchgate.net/profile/Tore-Dyba/publication/224266207_Recommended_Steps_for_Thematic_Synthesis_in_Software_Engineering/links/00b7d5342845262b79000000/Recommended-Steps-for-Thematic-Synthesis-in-Software-Engineering.pdf
ER  - 

TY  - JOUR
TI  - An introduction to quantum machine learning
AU  - Schuld, M.
AU  - Sinayskiy, I.
AU  - Petruccione, F.
T2  - Contemporary Physics
AB  - Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessable way, and discusses the potential of a future theory of quantum learning.
DA  - 2015/04/03/
PY  - 2015
DO  - 10.1080/00107514.2014.964942
DP  - arXiv.org
VL  - 56
IS  - 2
SP  - 172
EP  - 185
J2  - Contemporary Physics
SN  - 0010-7514, 1366-5812
UR  - http://arxiv.org/abs/1409.3097
Y2  - 2024/01/28/12:23:24
L1  - https://arxiv.org/pdf/1409.3097.pdf
L2  - https://arxiv.org/abs/1409.3097
KW  - Quantum Physics
ER  - 

TY  - GEN
TI  - Machine learning \& artificial intelligence in the quantum domain
AU  - Dunjko, Vedran
AU  - Briegel, Hans J.
AB  - Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our "big data" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.
DA  - 2017/09/08/
PY  - 2017
DP  - arXiv.org
PB  - arXiv
UR  - http://arxiv.org/abs/1709.02779
Y2  - 2024/01/28/12:24:00
L1  - https://arxiv.org/pdf/1709.02779.pdf
L2  - https://arxiv.org/abs/1709.02779
KW  - Computer Science - Artificial Intelligence
KW  - Quantum Physics
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - Quantum Machine Learning
AU  - Biamonte, Jacob
AU  - Wittek, Peter
AU  - Pancotti, Nicola
AU  - Rebentrost, Patrick
AU  - Wiebe, Nathan
AU  - Lloyd, Seth
T2  - Nature
AB  - Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Since quantum systems produce counter-intuitive patterns believed not to be efficiently produced by classical systems, it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement concrete quantum software that offers such advantages. Recent work has made clear that the hardware and software challenges are still considerable but has also opened paths towards solutions.
DA  - 2017/09//
PY  - 2017
DO  - 10.1038/nature23474
DP  - arXiv.org
VL  - 549
IS  - 7671
SP  - 195
EP  - 202
J2  - Nature
SN  - 0028-0836, 1476-4687
UR  - http://arxiv.org/abs/1611.09347
Y2  - 2024/01/28/12:24:13
L1  - https://arxiv.org/pdf/1611.09347.pdf
L2  - https://arxiv.org/abs/1611.09347
KW  - Statistics - Machine Learning
KW  - Quantum Physics
KW  - Condensed Matter - Strongly Correlated Electrons
ER  - 

TY  - ELEC
TI  - Export BibTeX to clipboard
T2  - Zotero Forums
AB  - Zotero is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.
DA  - 2021/01/26/
PY  - 2021
LA  - en
UR  - https://forums.zotero.org/discussion/87285/export-bibtex-to-clipboard
Y2  - 2024/02/27/16:03:28
L2  - https://forums.zotero.org/discussion/87285/export-bibtex-to-clipboard
ER  - 

TY  - GEN
TI  - Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy
AU  - Rath, Minati
AU  - Date, Hema
AB  - This research explores the integration of quantum data embedding techniques into classical machine learning (ML) algorithms, aiming to assess the performance enhancements and computational implications across a spectrum of models. We explore various classical-to-quantum mapping methods, ranging from basis encoding, angle encoding to amplitude encoding for encoding classical data, we conducted an extensive empirical study encompassing popular ML algorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector Machines and ensemble methods like Random Forest, LightGBM, AdaBoost, and CatBoost. Our findings reveal that quantum data embedding contributes to improved classification accuracy and F1 scores, particularly notable in models that inherently benefit from enhanced feature representation. We observed nuanced effects on running time, with low-complexity models exhibiting moderate increases and more computationally intensive models experiencing discernible changes. Notably, ensemble methods demonstrated a favorable balance between performance gains and computational overhead. This study underscores the potential of quantum data embedding in enhancing classical ML models and emphasizes the importance of weighing performance improvements against computational costs. Future research directions may involve refining quantum encoding processes to optimize computational efficiency and exploring scalability for real-world applications. Our work contributes to the growing body of knowledge at the intersection of quantum computing and classical machine learning, offering insights for researchers and practitioners seeking to harness the advantages of quantum-inspired techniques in practical scenarios.
DA  - 2023/11/17/
PY  - 2023
DP  - arXiv.org
PB  - arXiv
ST  - Quantum Data Encoding
UR  - http://arxiv.org/abs/2311.10375
Y2  - 2024/03/07/12:25:41
L1  - https://arxiv.org/pdf/2311.10375.pdf
L2  - https://arxiv.org/abs/2311.10375
KW  - Computer Science - Artificial Intelligence
KW  - Quantum Physics
ER  - 

TY  - ELEC
TI  - ZZFeatureMap
T2  - IBM Quantum Documentation
AB  - API reference for qiskit.circuit.library.ZZFeatureMap
LA  - en
UR  - https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap
Y2  - 2024/03/07/12:31:47
L2  - https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap
ER  - 

TY  - JOUR
TI  - Practical quantum advantage in quantum simulation
AU  - Daley, Andrew J.
AU  - Bloch, Immanuel
AU  - Kokail, Christian
AU  - Flannigan, Stuart
AU  - Pearson, Natalie
AU  - Troyer, Matthias
AU  - Zoller, Peter
T2  - Nature
DA  - 2022/07/28/
PY  - 2022
DO  - 10.1038/s41586-022-04940-6
DP  - DOI.org (Crossref)
VL  - 607
IS  - 7920
SP  - 667
EP  - 676
J2  - Nature
LA  - en
SN  - 0028-0836, 1476-4687
UR  - https://www.nature.com/articles/s41586-022-04940-6
Y2  - 2024/03/07/13:12:15
L1  - https://strathprints.strath.ac.uk/82197/1/Daley_etal_2022_Practical_quantum_advantage_in_quantum_simulation.pdf
ER  - 

TY  - JOUR
TI  - Geometric deep learning: going beyond Euclidean data
AU  - Bronstein, Michael M.
AU  - Bruna, Joan
AU  - LeCun, Yann
AU  - Szlam, Arthur
AU  - Vandergheynst, Pierre
T2  - IEEE Signal Processing Magazine
AB  - Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.
DA  - 2017/07//
PY  - 2017
DO  - 10.1109/MSP.2017.2693418
DP  - arXiv.org
VL  - 34
IS  - 4
SP  - 18
EP  - 42
J2  - IEEE Signal Process. Mag.
LA  - en
SN  - 1053-5888, 1558-0792
ST  - Geometric deep learning
UR  - http://arxiv.org/abs/1611.08097
Y2  - 2024/03/08/13:44:39
L1  - https://arxiv.org/pdf/1611.08097.pdf%C3%A2%E2%82%AC%E2%80%B9%C3%A2%E2%82%AC%E2%80%B9
KW  - Computer Science - Computer Vision and Pattern Recognition
ER  - 

TY  - JOUR
TI  - A review on quantum computing and deep learning algorithms and their applications
AU  - Valdez, Fevrier
AU  - Melin, Patricia
T2  - Soft Computing
AB  - In this paper, we describe a review concerning the Quantum Computing (QC) and Deep Learning (DL) areas and their applications in Computational Intelligence (CI). Quantum algorithms (QAs), engage the rules of quantum mechanics to solve problems using quantum information, where the quantum information is concerning the state of a quantum system, which can be manipulated using quantum information algorithms and other processing techniques. Nowadays, many QAs have been proposed, whose general conclusion is that using the effects of quantum mechanics results in a signiﬁcant speedup (exponential, polynomial, super polynomial) over the traditional algorithms. This implies that some complex problems currently intractable with traditional algorithms can be solved with QA. On the other hand, DL algorithms offer what is known as machine learning techniques. DL is concerned with teaching a computer to ﬁlter inputs through layers to learn how to predict and classify information. Observations can be in the form of plain text, images, or sound. The inspiration for deep learning is the way that the human brain ﬁlters information. Therefore, in this research, we analyzed these two areas to observe the most relevant works and applications developed by the researchers in the world.
DA  - 2023/09//
PY  - 2023
DO  - 10.1007/s00500-022-07037-4
DP  - DOI.org (Crossref)
VL  - 27
IS  - 18
SP  - 13217
EP  - 13236
J2  - Soft Comput
LA  - en
SN  - 1432-7643, 1433-7479
UR  - https://link.springer.com/10.1007/s00500-022-07037-4
Y2  - 2024/03/08/14:16:16
ER  - 

TY  - CONF
TI  - Convolutional Networks on Graphs for Learning Molecular Fingerprints
AU  - Duvenaud, David K
AU  - Maclaurin, Dougal
AU  - Iparraguirre, Jorge
AU  - Bombarell, Rafael
AU  - Hirzel, Timothy
AU  - Aspuru-Guzik, Alan
AU  - Adams, Ryan P
AB  - We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.
C3  - Advances in Neural Information Processing Systems
DA  - 2015///
PY  - 2015
DP  - Neural Information Processing Systems
VL  - 28
PB  - Curran Associates, Inc.
UR  - https://proceedings.neurips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html
Y2  - 2024/03/08/14:27:27
L1  - https://proceedings.neurips.cc/paper_files/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf
ER  - 

